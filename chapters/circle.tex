%%-------------------------process on a circle---------------------------------------%%
% \blue{
% 	discuss about data generation on a circle, 
% 	\begin{itemize}
% 		\item including circulant matrix
% 		\item why circulant matrix
% 		\item discuss covarince, biasness and the difficulties of estimation
% 		\item discuss variogram
% 		\item jones 1963
% 	\end{itemize}
% }

%%------------------------------------------------------------------%%
\section{Stationary process on a circle}
%%------------------------------------------------------------------%%

In this chapter we consider a real valued process $\{X(P): P\in S\}$ on the unit circle $S$, with finite second moment and continuity in quadratic mean. According to \cite{DUFOUR1976107} the process $\{X(P)\}$ can be represented in a Fourier series which is convergent in quadratic mean,

\beq
X(P) = A_0 + \sum_{n = 1}^\infty (A_n \cos(nP) + B_n \sin(nP).
\eeq


\begin{eqnarray}
	\nonumber
	\mbox{where} \quad A_0 &=&  \frac{1}{2\pi} \int_S X(P)dP, \\ \nonumber
	A_n &=& \frac{1}{\pi} \int_S \cos(nP)dP \\ 
	B_n &=& \frac{1}{\pi} \int_S \sin(nP)dP 
\end{eqnarray}

Let $P,Q$ be any two points on the circle, the covariance $R(P,Q)$ between the two points can be defined as,

\[
	R(P,Q) = E(X(P)X(Q)) = cov(X(P), X(Q))
\]

The process $X(P)$ is stationary if $E(X(P))$ is a constant and $R(P,Q)$ is function of the angular distance $\theta_{PQ}$ between $P$ and $Q$. If the process $X(P)$ is stationary on the circle,

\beq
cov(A_n, A_m) = a_n \delta(n,m) = cov(B_n, B_m), \quad \mbox{for $n, m \ge 0$}.
\eeq\\


Let $\{X(t_k)\}$ be a collection of gridded observations on a circle, with $t_k = (k-1)*2\pi/n, k = 1, 2, \cdots, n$. Let $C(\theta), \theta \in [0, \pi ]$ denote a stationary covariance function on the circle, the underlying process is stationary if it's covariance function solely depends on the distance $\theta$ and given as follows,

\beq
C(\theta) = cov(X(t_k+\theta), X(t_k)), \quad \quad \theta \in [0, \pi].
\eeq

The above covariance function can we can be written as a Fourier summmation (\cite{Roy1972})  

\beq
C(\theta) = \sum_{n=0}^\infty a_n \cos(n \theta),
\eeq

with $\sum_{n = 0}^\infty a_n < \infty$, and $a_n \ge 0$. Note that

\beq \nonumber
a_n  =\frac{2}{\pi}\int_0^\pi C(\theta) \cos(n\theta)d\theta \quad \mbox{and} \quad a_0  =\frac{1}{\pi}\int_0^\pi C(\theta)d\theta  .
\eeq 




%%------------------------------------------------------------------%%
\section{Estimation}
%%------------------------------------------------------------------%%

% The underlying process is stationary, if it's covariance function solely depends on the distance $\theta$,
% \beq
% C(\theta) = cov(X(t_k+\theta), X(t_k)), \quad \quad \theta \in [0, \pi].
% \eeq


Assuming the covariance function $C(\theta)$ on a cricle is a continuous function on $[0, \pi]$) and the gridded points $\{X(t_k)\}$ on a circle can be represented as $\utilde{X} = (X_1, X_2, \cdots, X_n)^T$. The variance-covariance matrix of the sample vector $\utilde{X} $ is given by $\Sigma$. Lets assume $E(X(t)) = \mu$ is unknown, and estimated by $\bar{X} = \frac{1}{n}\sum_{i=1}^{n} X(t_i)$.  Further, we can denote $\bar{X}$ in the following form,

\[
	\bar{X} = \frac{1}{n}{\bf 1}_n^T \utilde{X}
\]

\begin{eqnarray}
	\mbox{then,} \quad var(\bar{X}) &=& cov(\frac{1}{n}{\bf 1}_n^T \utilde{X}, \frac{1}{n}{\bf 1}_n^T \utilde{X}) \nonumber \\
	&=& \frac{1}{n^2}{\bf 1}_n^T \Sigma {\bf 1}_n \nonumber \\
	&=& \frac{1}{n} \left(C(0)+C(\pi)+2 \sum_{m=1}^{N-1}C(m 2\pi/n)\right) \nonumber
\end{eqnarray}


\begin{eqnarray*}
\mbox{When $n \to \infty$,}	\frac{1}{n} \left(2 \sum_{m=0}^{N}C(m 2\pi/n)\right) &=& \frac{1}{\pi} \frac{\pi}{N} \left( \sum_{m=0}^{N}C(m 2\pi/n)\right) \\ 
	&\to& \frac{1}{\pi} \int_0^\pi C(\theta)d\theta = a_0.
\end{eqnarray*}

\begin{eqnarray*}
	\mbox{Hence} \quad	var(\bar{X}) &=& \frac{1}{n} \left(2 \sum_{m=0}^{N}C(m 2\pi/n)\right) - \frac{1}{n}(C(0) + C(\pi)) \\
	&\to & a_0, \quad \quad \mbox{as $n \to \infty$}.
\end{eqnarray*}

 
\[
	\mbox{We can conclude that }	var(\bar{X}) \to \frac{1}{\pi} \int_0^\pi C(\theta)d\theta \quad \mbox{as $n \to \infty$}.  
\]


\begin{prop}

  Let $\bar{X}$ be an unbiased estimator for $\mu$ then $\bar{X}$ is not a consistent estimator for $\mu$, mean on a circle. \\
  
  {\bf proof:}
	If $\bar{X}$ is consistant we get the following, 
	\[
		P(|\bar{X} - \mu| > \varepsilon) \to 0.
	\]

	If $var(\bar{X}) \to 0$ and from Chebyshev's inequality we have

	\[
		P(|\bar{X} - \mu| > \varepsilon) \le \frac{var(\bar{X})}{\varepsilon^2} \to 0, \quad \mbox{for any $\varepsilon > 0$}.
	\]

	Therefore, $var(\bar{X}) \to 0$ is a sufficient condition for consistency, but it is not necessary. However, if we assume $\utilde{X}$ is multivariate normally distributed, then $\bar{X}$ follows normal distribution with mean $\mu$ and approximate variance $\sigma_0$. Then ($Z \sim N(0, 1)$)
	 
	 \begin{eqnarray*}
		P(|\bar{X} - \mu| > \varepsilon) &=& P\left(\frac{|\bar{X} - \mu|}{\sqrt{a_0}} > \frac{\varepsilon}{\sqrt{a_0}}\right) \\
		&=& P\left(|Z| > \frac{\varepsilon}{\sqrt{\sigma_0}}\right) \not\to 0  
	\end{eqnarray*}
	
	since $\frac{\varepsilon}{\sqrt{\sigma_0}}$ is a fixed constant for each fixed $\varepsilon > 0$.\\
	
\end{prop}

%-------------------------------------% 
\subsection{Estimation of covariance on a circle} \label{est_covariance}
%-------------------------------------%

We used method of moments (MOM) to estimate the covariance $C(\theta)$ on a circle, the estimator can be given by

\beq \label{covarince_estimator}
\hat{C}(\Delta \lambda) = \frac{1}{n}\sum_{i = 1}^n (X(t_i + \Delta \lambda) - \bar{X})(X(t_i) - \bar{X}), 
\eeq

where $\Delta \lambda = 0, 2\pi/n, 4\pi/n, \cdots, 2(N-1)\pi/n$.\\

We can show that the above estimator is a biased estimator for $C(\theta)$ .

\begin{eqnarray}
	\nonumber
	E(\hat{C}(\Delta \lambda)) &=& \frac{1}{n}\sum_{i = 1}^n E((X(t_i + \Delta \lambda) - \bar{X})(X(t_i) - \bar{X})) \\ \nonumber
	&=& \frac{1}{n}\sum_{i = 1}^n E((X(t_i + \Delta \lambda) - \mu - (\bar{X} - \mu))(X(t_i) -\mu - (\bar{X}) - \mu)) \\ \nonumber
	&=& \frac{1}{n}\sum_{i=1}^n cov(X(t_i+\Delta \lambda), X(t_i)) - \frac{1}{n}\sum_{i = 1}^n E((X(t_i + \Delta \lambda) - \mu)(\bar{X} - \mu)) \\ \nonumber
	& & -\frac{1}{n}\sum_{i = 1}^n E((X(t_i) - \mu)(\bar{X} - \mu)) + \frac{1}{n}\sum_{i = 1}^n E((\bar{X} - \mu)(\bar{X} - \mu)) \\ \nonumber
	&=& C(\Delta \lambda) -E((\bar{X} - \mu)(\bar{X} - \mu)) - E((\bar{X} - \mu)(\bar{X} - \mu)) \\ \nonumber 
	& & + E((\bar{X} - \mu)(\bar{X} - \mu)) \\ \nonumber
	&=& C(\Delta \lambda) - var(\bar{X}).
\end{eqnarray}

% Moreover,
% 
% \begin{eqnarray*}
% 	\nonumber
% 	var(\bar{X}) &=& E((\bar{X} - \mu)(\bar{X} - \mu)) = \frac{1}{n^2}\sum_{i = 1}^n \sum_{j=1}^n E(X(t_i) - \mu)(X(t_j) - \mu) \\ \nonumber
% 	&=&  \frac{1}{n^2}\sum_{i = 1}^n \sum_{j=1}^n cov(X(t_i), X(t_j)) = \frac{1}{n^2}\sum_{i = 1}^n \sum_{j=1}^n C(m*(i-j)*2\pi/n) \\ \nonumber
% 	&=& \frac{1}{n^2}\sum_{i = 1}^n \sum_{j=1}^n \left(a_0 + \sum_{k=1}^\infty a_k \cos(m*(i-j)*2\pi/n)\right) \\ \nonumber
% 	&=& a_0 + \sum_{k=1}^\infty a_k \left(\frac{1}{n^2}\sum_{i = 1}^n \sum_{j=1}^n \cos(m*(i-j)*2\pi/n)\right).
% \end{eqnarray*}
% 
% Now,
% 
% \begin{eqnarray*}
% 	& & \sum_{i = 1}^n \sum_{j=1}^n \cos(m*(i-j)*2\pi/n) \\
% 	&=& \sum_{i=1}^n \sum_{j=1}^n \left(\cos(m*i *2\pi/n)\cos(m*j*2\pi/n) + \sin(m*i *2\pi/n)\sin(m*j*2\pi/n) \right)\\
% 	&=& \left(\sum_{i=1}^n \cos(m*i *2\pi/n)\right)^2 + \left(\sum_{i=1}^n \sin(m*i *2\pi/n)\right)^2 = n^2
% \end{eqnarray*}
% 
% since for any integer $m$, we have
% 
% \[
% 	\sum_{k = 1}^{n} \cos(mk*2\pi/n) = \left\{\begin{array}{cc}
% 	0, & \mbox{for any integer $m \ne 0$,}  \\
% 	n, & \mbox{for $m = 0$}
% 	\end{array}
% 	\right. \mbox{ and }
% 	\sum_{k = 1}^{n} \sin(mk*2\pi/n) = 0.
% \]
% 
% Hence,
% \[
% 	var(\bar{X}) = a_0.
% \]
% 
% Therefore,
% \beq
% E(\hat{C}(\Delta \lambda)) = C(\Delta \lambda) - a_0.
% \eeq



% {\bf Note}: When $var(\bar{X}) \not\to 0$, we can not make the conclusion that $\bar{X}$ is not consistent. For consistency, we need to prove that


{\bf Remark 1} The MOM estimator $\hat{C}(\Delta \lambda)$ of the covariance function is actually a biased estimator with the shift amount of $a_0$. Therefore, if $a_0 = 0$ for a covariance function, we have the unbiased estimator $\hat{C}(\Delta \lambda)$. \\

{\bf Remark 2} If the gridded points were on a line, for example in time series, $E(\bar{X} - \mu)^2 \to 0$ as $n \to \infty$ under the assumption that the covariance function $C(\theta) \to 0$ when $\theta \to \infty$ (which is practically feasible), that is, $\bar{X}$ is consistent in the case of points on a line. In the case of circle, we might not have $C(\theta)$ close to 0 since $\theta$ is within a bounded region ($(0, \pi)$ for the circle) and we normally assume $C(\theta)$ is continuous for $\theta$. \\


%-------------------------------------%
\subsection{Estimation of variogram on a circle}
%-------------------------------------%

The theoretical variogram function is given by,
\beq
\gamma(\theta) = C(0) - C(\theta).
\eeq
and the MOM estimator for the variogram is given by,

\beq
\hat{\gamma}(\Delta \lambda) = \frac{1}{2n} \sum_{i=1}^n (X(t_i + \Delta \lambda) - X(t_i))^2.
\eeq

We can show that variogram estimator through MOM is an unbiased estimator, 

\begin{eqnarray*}
	E(\hat{\gamma}(\Delta \lambda)) &=& \frac{1}{2n} \sum_{i = 1}^n E(X(t_i + \Delta \lambda) - X(t_i))^2 \\
	&=& \frac{1}{2n} \sum_{i = 1}^n E((X(t_i + \Delta \lambda)-\mu) - (X(t_i) - \mu))^2 \\
	&=& \frac{1}{2n} \sum_{i = 1}^n cov(X(t_i + \Delta \lambda) - X(t_i), X(t_i + \Delta \lambda) - X(t_i)) \\
	&=& \frac{1}{2n} \sum_{i = 1}^n \left\{ cov(X(t_i + \Delta \lambda), X(t_i + \Delta \lambda)) + cov(X(t_i), X(t_i)) \right. \\
	& & \left. - 2cov(X(t_i + \Delta \lambda), X(t_i)) \right\}\\
	&=& \frac{1}{2n} \sum_{i = 1}^n \left( C(0) + C(0) - 2C(\Delta \lambda)\right) \\
	&=& C(0) - C(\Delta \lambda) = \gamma(\Delta \lambda).
\end{eqnarray*}
 
\blue{need to prove consistency}


%-------------------------------------%
\section{Data generation on a circle}
%-------------------------------------% 



First, we will discuss how to generate correlated data at $n$ girdded points on a circle when the covarince function is defined and compare above covariance and variogram estimators. Since the observed data are correlated, the covariance function can be written as a function of distance (angle). For the data generation process we will use exponential family and power family covariance function as given below, 

\beq \label{exp_cov} 
C(\theta) = C_1e^{-a|\theta|} \quad a>0, C_1>0
\eeq

\beq \label{power_cov} 
C(\theta) = c_0 - (|\theta|/a)^{\alpha} \quad a>0, \alpha \in (0,2] \mbox{ and } c_0 \ge \int_0^\pi
			(\theta/a)^{\alpha} \sin \theta d \theta
\eeq


where $\theta = i*\Delta\lambda = \pm i*2\pi/n, i=1,2,\cdots,\floor{n/2}$ \\

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{graphs/process_circle}
	\caption {Random process on a circle at 24 points ($\Delta\lambda = 15^0$), the red dots represent the  observed values at a given time and each point is associated with a random process of it's own.}
\end{figure}

Clearly, each location is correlated with other $n-1$ locations and $C(\theta) = C(-\theta)$ the variance-covariance matrix $\Sigma$ is circulant and will be in the following form,   


\begin{eqnarray*}
	\Sigma &=& circ(C(0),C(\delta), C(2\delta), \cdots, C((N-1)\delta), C(\pi),  C((N-1)\delta), \cdots, C(\delta)) \\
	&=& \left(\begin{array}{ccccccc}
	C(0)      & \cdots & C((N-1)\delta ) & C(\pi) &  C((N-1)\delta ) & \cdots & C(\delta) \\
	C(\delta) & \cdots & C((N-2)\delta) & C((N-1)\delta) &  C(\pi)  & \cdots & C(2\delta) \\
	C(2\delta) & \cdots & C((N-3)\delta) & C((N-2)\delta) &  C((N-1)\delta) & \cdots & C(3\delta)\\
	\vdots    & \vdots  & \vdots  & \vdots  & \vdots  & \vdots  & \vdots  \\
	C(\delta) & \cdots & C(\pi) &  C((N-1)\delta) & C((N-2)\delta)  & \cdots & C(0) 
	\end{array} \right) \\
	&=& Q\Lambda Q^T,
\end{eqnarray*}

where $\delta = 2\pi/n,\ \Lambda=\{\lambda_1, \lambda_2,\cdots,\lambda_n\}$ and $Q=\{\psi_1, \psi_2,\cdots,\psi_n\}$ are the respective eigen values and eigen vectors of the above circulant matrix. Now using singular value decomposition (SVD) we can obtain the correlated data $\utilde{X}$ on a circle as follows,

\[
	\utilde{X} = \Sigma^{1/2}*Z = Q\Lambda^{1/2}Q^T*Z 
\]

where $Z\sim N(\utilde{0},1_{n})$.

%-------------------------------------%
\subsection{Compare covariance estimator} 
%-------------------------------------%

% \beq\label{cov:circle1}
% C(\theta) = \frac{1}{n_L} \sum_{i=1}^{n_L} (X(a_i+\theta)\cdot X(a_i))-(\overline{X(a)})^2
% \eeq

	      In section \ref{est_covariance} we proved that, in general the covarince estimator (\ref{covarince_estimator}) on a circle is biased, with a bias of $var(\bar{X})$. In order to make things simple we set $C_1, a = 1$ when $\alpha = 0.5$ $c_0 \ge \int_0^\pi(\theta)^{0.5} \sin \theta d \theta$, from Fresnel intergal it can be shown that $c_0 \ge 2.4353$. Now we compare the covariance estimator to it's theoretical covarince given by equations \ref{exp_cov} and \ref{power_cov}. We computed the MOM estimator $\hat{C}(\theta)$ with 48 gridded observations on the circle from 500 simulations.
	      	      
	      \begin{figure}[H]
	      	\label{covarince_circle}
	      	\centering
	       	\includegraphics[width=0.9\textwidth]{graphs/covarince_circle}
	      	\caption {Theoretical and empirical covariance (with bias) comparison on a circle}
	      \end{figure}

	      {\bf Remark 1:} We noticed that the shift between theoretical and empirical values were approximately equal to $a_0$ and we can obtain $a_0$ for the above covarince as follows,
	      \[
	    \mbox{exponential : }  	a_0 = \frac{C_1}{a\pi}(1 - e^{-a\pi})
	      \]

	      \[
	    \mbox{power : }  	a_0 = c_0 - \left(\frac{\pi}{a}\right)^{\alpha}\frac{1}{\alpha + 1}
	      \]

	      Now we consider the following covariance function, after subtracting $a_0$ from $C(\theta)$.
	      \[
	      	D(\theta) = C(\theta) - a_0.
	      \]
      
        If the new covariance function $D(\theta)$ was used to generate the data on a circle then the theoretical and empirical values will match perfectly.
        
	      % \beq \label{cov:circle1}  
	      % C(\theta) = \frac{1}{n_L} \sum_{i=1}^{n_L} (X(a_i+\theta) \cdot X(a_i)) 
	      % \eeq
	      	      
	      \begin{figure}[H]
	      	\centering
	      	\includegraphics[width=0.9\textwidth]{graphs/covarince_circle_remove_a0}
	      	%graph from data genaration summary doc line 194 
	      	\caption {Theoretical and empirical covariance comparison on a circle}
	      \end{figure}
	      	      
{\bf Remark 2:} If the process is a zero mean process the covariance estimator will be unbiased ($i.e. Var(\bar{X}) = 0$) hence we will get a perfect match between theoretical and empirical values.\\

{\bf Remark 3:} We have shown that covariance estiamtor is biased and the biasness will approach to $a_0$. When covariance function is unknown the biaseness $a_0$ is also known and the biasness cannnot to estimated (cannot find the variance of $\bar{X}$) from one circle, however in both exponential and power covariance models it can be estimated if multiple copies of data (on a circle) were generated {\em i.e.} $\hat{a} = var(\bar{X})$. 

	      \begin{figure}[H]
	      	\centering
	      	\includegraphics[width=0.9\textwidth]{graphs/covarince_circle_estimate_a0}
	      	%graph from data genaration summary doc line 194 
	      	\caption {Theoretical and empirical covariance comparison on a circle, after removing biasness}
	      \end{figure}
	      

	      	      
%-------------------------------------%
\subsection{Compare variogram estimator} 
%-------------------------------------%

We proved that in general the variogram estimator is unbiased \blue{and consistent}. Since the semi variogram $\gamma(\theta) = C(0) - C(\theta)$,  the theoretical variogram based on exponential and power covariance functions can be given in the following form,   

\[
	\mbox{exponential : }\gamma(\theta) = C(0) - C(\theta) = C_1(1-e^{-a|\theta|})
\]

\[
	\mbox{power : } \gamma(\theta) = C(0) - C(\theta) = C_1(1-e^{-a|\theta|})
\]

We computed the variogram estimator $\hat{\gamma}(\theta)$ with 48 gridded observations on the circle from 500 simulations and there is a better fit between theoretical and empirical values compared to covariance models.


\begin{figure}[H]
	\centering
	%\includegraphics[width=0.65\textwidth]{graphs/Summary-covarince_circle_1}
	%\includegraphics[width=0.65\textwidth]{graphs/variogram_plot_4000}
	\includegraphics[width=0.9\textwidth]{graphs/variogram_plot_500}
	%graph from data genaration summary doc line 177 
	\caption {Theoretical and empirical comparison for variogram on a circle}
\end{figure}

{\bf Remark 4} \blue{should we talk about how $n_L$ is related with number of simulations }

