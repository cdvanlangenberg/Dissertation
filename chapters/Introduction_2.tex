%%-------------------------Introduction---------------------------------------%%


% \input{template.tex}

% \begin{document}

In this chapter we will give a brief introduction to some of the basic concepts in spatial statistics, which are necessary to follow for the rest of this dissertation. More specifically, we will discuss stationarity and intrinsic stationarity, covariance and variogram functions and their properties, mean square continuity and differentiability, spectral representations and spectral densities, complex random processes and Gaussian random vectors, as well as some basic properties related to circulant and block circulant matrices.   %\\~\\


%%------------------------------------------------------------------%%
\section{Spatial random field}
%%------------------------------------------------------------------%%
A random process is a collection of random variables $\{Z(\X): \X \in \Omega\}$, defined in a common probability space that takes values on a specific domain $\Omega$. Generally, $\Omega$ may take a variety of forms as given below. 

\begin{itemize}
	\item $\X \in \Omega = N$: $Z(\X)$ is a time series.
	\item $\X \in \Omega = \R^1$: $Z(\X)$ is a random process, commonly referred as a stochastic process.
	\item $\X \in \Omega = \R^d$: $Z(\X)$ is a random field or a spatial process if $d > 1$.
	\item $\X \in \Omega = \Sp^2$: $Z(\X)$ is a random process on the sphere.
	\item $\X \in \Omega = \R^d\times R$: $Z(\X)$ is a spatio-temporal process that involves both location and time.
\end{itemize}

We now denote $\{Z(x): x \in D \subset \mathbb{R}^d\}$ as a real-valued spatial random field in $d$-dimensional Euclidean space $R^d$, where $x$ is the location, varying over a fixed domain $D$. The distribution of $Z(x)$ is characterized by its finite-dimensional distribution function, that is, the distribution function of the random vector $ Z(\utilde{\X})=(Z(\X_1),\ldots, Z(\X_n) )$ given by
\beq
% F\{Z(h_1),\ldots, Z(h_n)\} = P\{ Z(x_1)\le h_1,\ldots, Z(x_n)\le h_n \},
F(h_1,\ldots, h_n) = P( Z(x_1)\le h_1,\ldots, Z(x_n)\le h_n ),
\eeq
for any $n$ and any sequence of locations $(x_1, x_2, \ldots, x_n)$ and $h_1,\ldots, h_n \in \R$.\\


%-------------------------------------%
\subsection{Stationarity and Isotropy}
%-------------------------------------%

A spatial random field $Z(x)$ is said to be strictly stationary, if for any $n,\ \xn \in \mathbb{R}^d$, $h_1, \ldots, h_n\in\mathbb{R} \mbox{ and } x\in \mathbb{R}^d$, $Z(x)$ is invariant under translation, that is,

\beq
P(Z(x_1+x)\le h_1, \ldots, Z(x_n+x)\le h_n) = P( Z(x_1)\le h_1,\ldots, Z(x_n)\le h_n)
\eeq

Strict stationarity is normally too strong a condition as it involves the distribution of the random field. Another commonly used but weaker assumption is the weak stationarity. More specifically, A random process $Z(x)$ is weakly stationary if
\begin{eqnarray}
	E(Z(x))   & = & \mu \nonumber \\
	E(Z^2(x)) & < & \infty \nonumber \\
	C(h)      & = & Cov(Z(x),Z(x+h))
\end{eqnarray}

In other words a random process $Z(x)$ is weakly stationary (or simply stationarity throughout the rest of this dissertation) if it has constant mean and finite second moment as well as its (auto-)covariance function $C(h)$ solely depends on the spatial distance of two locations. Further, a strictly stationary random field with finite second moment is weakly stationary, but weak stationarity does not imply strict stationarity unless $Z(x)$ is a Gaussian random field, under which both stationarities are equivalent, as the finite-dimensional distribution of a Gaussian random field is multivariate normal and, which is uniquely determined by the first and second moments. \\

The covariance function $C(h)$ of a stationary process $Z(x)$ on $\R^d$ has the following properties.

\begin{enumerate}[(i)]
	\item $C(0) \ge 0$;
	\item $C(h) = C(-h)$;
	\item $|C(h)| \le  C(0)$;
	\item If $C_1(h), C_2(h), \ldots, C_n(h)$ are valid covariance functions, then each of the following functions $C(h)$ is also a valid covariance function.
	
	      \begin{enumerate}
	      	\item $C(h) = a_1C_1(h)+a_2C_2(h)$, $\forall a_1,a_2\ge 0$;
	      	\item $C(h) = C_1(h)C_2(h)$;
	      	\item $\underset{n\to\infty} {\lim}\ C_n(h)=C(h),\ \forall h\in \R^d$.
	      \end{enumerate}
	
\end{enumerate}

A function $C(\cdot)$ on $\R^d$ is non-negative definite if and only if

\beq 
\label{cov_pd}
\sum_{i,j=1}^{N} a_i a_j C(x_i - x_j) \ge 0,
\eeq

for any integer $N$, any constants $a_1, a_2, \ldots, a_N$, and any locations $x_1, x_2, \ldots, x_N \in \R^d$. 

% Positive definiteness is a necessary and sufficient condition such that a function is a valid covariance function. \\

A valid \cov must be positive definite. On the other hand, given a \pd function, one can always define a family $Z(x), x \in D$ of zero-mean Gaussian random process with the given function as its \cov function.

		
A weakly stationary process with a covariance function $C(||h||)$ which is free from direction is called isotropy. The random field, $Z(x)$, on $\mathbb{R}^d$ is strictly isotropy if the joint distributions are invariant under all rigid motions. {\em i.e.,} for any orthogonal $d\times d$ matrix $H$ and any $x\in \R^d$

\beq
P(Z(Hx_1+x)\le h_1, \ldots, Z(Hx_n+x)\le h_n) = P( Z(x_1)\le h_1,\ldots, Z(x_n)\le h_n)
\eeq

Isotropy assumes that it is not required to distinguish one direction from another for the random field $Z(x)$. When describing the correlation between random fileds at two locations , that is, the correlation of $Z(\X)$ at any two locations is the same as long as these two points \blue{.......complete...........}

Variogram function is an alternative to the covariance function proposed by Matheron (1973). It is defined as the variance of the difference between random fields at two locations, that is

\begin{align}
	2\gamma(h) & = Var(Z(\X+h) -Z(\X)).
\end{align}

Here $\gamma(h)$ is called the semivariogram. If the variogram function solely depends on the distance of the two locations, then the process with finite constant mean is said to be intrinsically stationary. If $Z(x)$ is further assumed to be stationary with covariance function $C(h)$, then $\gamma(h) = C(0) - C(h)$. Intrinsic stationarity is defined in terms of variogram and it is more general than (weak) stationarity that is defined in terms of covariance. Clearly, when $C(h)$ is known, we can obtain $\gamma(h)$ but the reverse is not true. For example we consider a linear semivariogram function given below,
\[
	\gamma(h) = \left \{ \begin{array}{cc}
	a^2+\sigma^2h & h>0 \\
	0 & otherwise \\
	\end{array}
	\right.
\]

% \underset{h \to \infty} {\lim}

when $\gamma(h) \to \infty$ as $h \to \infty$ thus the process with the above semivariogram is not weakly stationary and $C(h)$ does not exist. \\

Parallel to the positive definiteness for the covariance function, the variogram is conditionally negative definite, that is,
\beq
\sum_{i,j=1}^{N} a_i a_j 2\gamma(x_i - x_j) \le 0,
\eeq
for any integer $N$, any constants $a_1, a_2, \ldots, a_N$ with $\sum_{i=1}^N a_i = 0$, and any locations $x_1, x_2, \ldots, x_N \in \mathbb{R}^d$.

%\blue{spectral representation of variogram}

%-------------------------------------%
\subsection{\bf Mean square continuity}
%-------------------------------------%

For a sequence of random variables $X_1, X_2,\ldots$ and a random variable $X$ defined on a common probability space, define $X_n\overset{L^2}\to X$ if $E(X^2)<\infty$ and $E(X_n - X)^2\to 0$ as $n \rightarrow \infty$. We then say $\{X_n\}$ converges in $L^2$ to $X$ if there exists such a $X$.\\

There is no simple relationship between $C(h)$ and the smoothness of $Z(x)$. Suppose $Z(x)$ is a random field on $\R^d$, then $Z(x)$ is mean square continuous at $x$ if
\[
\lim_{h\to 0} E(Z(x+h)-Z(x))^2 =0.
\]
If $Z(x)$ is stationary and $C(\cdot)$ is its covariance function then $E(Z(x+h)-Z(x))^2=2(C(0)-C(h))$. Therefore $Z(x)$ is mean square continuous if and only if $C(\cdot)$ is continuous at the origin.

%-------------------------------------%
\subsection{\bf Spectral representation of a random field}
%-------------------------------------%

Suppose $\omega_1,\ldots, \omega_n \in \R^d$ and let $Z_1, \ldots, Z_n$ be mean zero complex-valued random variables with  $E(Z_i\bar{Z_j})=0$  ($\bar{a}$ represents the conjugate of the complex number $a$), $i\ne j$ and  $E|Z_i|^2=f_i$. Then the random sum

\beq 
Z(x) = \sum_{k=1}^n Z_k e^{i\omega_k^Tx}, \quad x\in\R^d
\eeq

is a weakly stationary complex random field in $\R^d$ with possibly complex-valued covariance function $C(x) = \sum_{k=1}^n f_k e^{i\omega_k^Tx}$.

In spatial statistics, sometimes it is more convenient to use complex valued random functions, rather than real valued random functions. We say, $Z(x)=U(x) + i V(x)$ is a complex random field if $U(x),V(x)$ are real random fields. If $U(x),V(x)$ are stationary so does $Z(x)$. The covariance function can be defined as,

\[
	C(h) = cov(Z(x+h), \overline{Z(x)}), \quad C(-h)=\overline{C(h)}.
\]

For any complex constants $c_1,\ldots, c_n,$ and any locations $x_1, x_2, \ldots, x_n$,

\beq
\sum_{i,j=1}^n c_i\bar{c_j}C(x_i-x_j)\ge 0
\eeq

Further, if we consider the integral as a limit in $L^2$ of the above random sum, then the covariance function can be represented as,

\beq \label{cov_spectral}
C(x) = \int_{\mathbb{R}^d} e^{i\omega^Tx} F(d\omega)
\eeq
where $F$ is the so-called spectral distribution. Here is a more general result from Bochner,

%-------------------------------------%
\begin{theorem}[{\bf Bochner's Theorem}]\hfill \\
A complex valued covariance function $C(\cdot)$ on $\R^d$ for a weakly stationary mean square continuous complex-valued random field on $\mathbb{R}^d$ if and only if it can be represented as \eqref{cov_spectral} , where $F$ is a positive measure.
\end{theorem}

If $F$ has a density (spectral density, denoted by $f$ with respect to Lebesgue measure, ($i.e.$ if such a $f$ exists) we can use the inversion formula to obtain $f$
%-------------------------------------%


\beq
f(\omega) = \frac{1}{(2\pi)^d}  \int_{\mathbb{R}^d} e^{-i\omega^Tx} C(x) dx
\eeq

%-------------------------------------%
\subsection{\bf Spectral densities}
%-------------------------------------%

Here we provide some examples of isotropic covariance functions and their corresponding spectral densities.
\begin{enumerate}[(i)]
	\item Rational Functions that are even,  non-negative and integrable the corresponding covariance functions can be expressed in terms of elementary functions. For example if $f(\omega) =\phi (\alpha^2+\omega^2)^{-1}$, then $C(h) = \pi\phi\alpha^{-1}e^{-\alpha|h|}$ (obtained by contour integration).
	
	\item Gaussian are the most commonly used covariance function for a smooth process on $\mathbb{R}$ where the covariance function is given by $C(h)=ce^{-\alpha h^2}$ and the corresponding spectral density is $ f(\omega) = \frac{1}{2\sqrt{\pi\alpha}}c e^{\frac{-\omega^2}{4\alpha}}$.

%    \item Spherical covariance function is given by $C(h) = $ and its corresponding spectral density is $f(\omega) = $.
	
	\item $Mat\acute{e}rn$ class has more practical use and more frequently used in spatial statistics. The spectral density of the form $f(\omega) =\frac{1}{\phi(\alpha^2+\omega^2)^{\nu+1/2}}$ where $\phi,\nu,\alpha>0$ and the corresponding covariance function given by,
	
	      \beq
	      C(h) = \frac{\pi^{1/2}\phi}{2^{\nu-1}\Gamma(\nu+1/2)\alpha^{2\nu}} (\alpha|h|)^{\nu} Y_{\nu} (\alpha|t|)
	      \eeq
	
	      where $Y_{\nu}$ is the modified Bessel function, the larger the $\nu$ smoother the $Y$. Further, $Y$ will be $m$ times square differentiable iff $\ \nu>m$. When $\nu$ is in the form of $m+1/2$ with $m$ a non negative integer. The spectral density is rational and the covariance function is in the form of $e^{-\alpha|h|}\cdot$ polynomial$(|h|)$ for example, when $\nu=\frac{1}{2}$ $C(h)$ corresponds to exponential model and $\nu=\frac{3}{2}$ is transformation of exponential family of order 2.     
	
	      \begin{eqnarray*}
	      	\nu = 1/2 &:& C(h) = \pi\phi\alpha^{-1}e^{-\alpha|h|}\\
	      	\nu = 3/2 &:& C(h) = \frac{1}{2}\pi\phi\alpha^{-3}e^{-\alpha|h|}(1+\alpha|h|)\\
	      \end{eqnarray*}
	
\end{enumerate}

%%------------------------------------------------------------------%%
\section{Circularly-symmetric Gaussian random vectors}
%%------------------------------------------------------------------%%

Let $\utilde{Z} = (Z_1, Z_2, \ldots, Z_n)^T$, where $Z_j = (Z_j^{Re}, Z_j^{Im})^T$ and $j=1,2,\ldots, n$ be a zero mean $2n$ complex random vector of dimension $2n$. Then its covariance matrix $K_Z$ and the pseudo-covariance matrix $M_Z$ are defined as follow.

\begin{eqnarray} 
\label{complex_cov}
K_{\utilde{Z}} &=& E[\utilde{Z}\utilde{Z}^*]\\
\label{complex_pcov}
M_{\utilde{Z}} &=& E[\utilde{Z}\utilde{Z}^T]
\end{eqnarray}

where $\utilde{Z}^* = \utilde{\bar{Z}}^T$ is the conjugate transpose of $\utilde{Z}$.\\

Generally, to characterize the relationship of a complex random vector, one needs both covariance and pseudo-covariance matrices. First note that a complex random variable $Z = Z^{Re} + iZ^{Im}$ is (complex) Gaussian, if $Z^{Re}, Z^{Im}$ both are real and they are jointly Gaussian. Now we consider a vector $\utilde{Z}=(Z_1, Z_2)^T$ where $Z_1=Z_1^{Re }+i Z_1^{Im}$ and $Z_2=Z_1^*$ ($Z_2^{Re}=Z_1^{Re }, Z_2^{Im}=-Z_1^{Im}$). The four real and imaginary parts of $\utilde{Z}$ are jointly Gaussian (each follows $N(0,1/2)$) (so $\utilde{Z}$ is complex Gaussian). \\

The covariance matrices defined by \eqref{complex_cov} is given by

\[
M_Z = E \begin{bmatrix}
Z_1^2 & Z_1Z_1^*\\
Z_1Z_1^* & Z_1^2
\end{bmatrix} =
\begin{bmatrix}
0 & 1\\
1 & 0
\end{bmatrix}
\]

and the pseudo-covariance matrices defined by \eqref{complex_pcov} is given by

\[
K_Z= E \begin{bmatrix}
Z_1Z_1^*  & Z_1^2\\
Z_1^2     & Z_1Z_1^*
\end{bmatrix} =
\begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix}.
\]

It is easy to note that $E[Z_1^2] = E[Z_1^{Re}Z_1^{Re}-Z_1^{Im}Z_1^{Im}] = 1/2 -1/2 = 0$. If both $Z_1$ and $Z_2$ are real, then covariance and pseudo-covariance matrices are the same, {\em i.e.}, $M_Z \equiv K_Z$   \\

The covariance matrix of real $2n$ random vector $\utilde{Z}=(\utilde{Z}^{Re}, \utilde{Z}^{Im})^T$, where $\utilde{Z}^{Re} = (Z_1^{Re}, Z_1^{Re}, \ldots, Z_1^{Re})$ and $\utilde{Z}^{Im} = 
(Z_1^{Im}, Z_2^{Im}, \ldots, Z_n^{Im})$ can be determined by both $K_{\utilde{Z}}$ and $M_{\utilde{Z}}$ given as follow.

\begin{eqnarray}
	E[\utilde{Z}^{Re}\utilde{Z}^{Re}] &=& \frac{1}{2}Re(K_{\utilde{Z}} + M_{\utilde{Z}}), \nonumber\\
	E[\utilde{Z}^{Im}\utilde{Z}^{Im}] &=& \frac{1}{2}Re(K_{\utilde{Z}} - M_{\utilde{Z}}), \nonumber\\
	E[\utilde{Z}^{Re}\utilde{Z}^{Im}] &=& \frac{1}{2}Im(-K_{\utilde{Z}} + M_{\utilde{Z}}), \nonumber\\
	E[\utilde{Z}^{Im}\utilde{Z}^{Re}] &=& \frac{1}{2}Im(K_{\utilde{Z}} + M_{\utilde{Z}}) \label{comlex_cov}
\end{eqnarray}

We can get the covariance of $\utilde{Z}=(\utilde{Z}^{Re}, \utilde{Z}^{Im})^T$ as follows,

\begin{eqnarray*}
	Cov(\utilde{Z}) &=& E(\utilde{Z}\utilde{Z}^T) \\
	&=& \left( \begin{array}{ll}
	E[\utilde{Z}^{Re}\utilde{Z}^{Re}] &  E[\utilde{Z}^{Re}\utilde{Z}^{Im}] \\
	E[\utilde{Z}^{Im}\utilde{Z}^{Re}] &  E[\utilde{Z}^{Im}\utilde{Z}^{Im}]
	\end{array}
	\right) \\
\end{eqnarray*}

Now we introduce circularly-symmetric random variables and vectors. A complex random variable $Z$ is circularly-symmetric if both $Z$ and $e^{i\phi} Z$ have the same probability distribution for all real $\phi$.  Since $E[e^{i\phi}Z] = e^{i\phi}E[Z]$, any circularly-symmetric complex random variable must have $E[Z]=0$, in other words its mean must be zero. \\

For a circularly-symmetric complex random vector, we have the following theorem \cite{Gallager2008}.\\


\begin{theorem}[Gallager, 2008] \label{circular_theory} \hfill \\
	Let $\utilde{Z}$ be a zero mean Gaussian random vector then $M_{\utilde{Z}}=0$ if and only if $\utilde{Z}$ is circularly-symmetric.
\end{theorem}

\vskip 8pt

%%------------------------------------------------------------------%%
\section{Circulant matrix} \label{circulant}
%%------------------------------------------------------------------%%

A square matrix $A_{n\times n}$ is a circulant matrix if the elements of each row (except first row) has the previous row shifted by one place to the right.

\begin{eqnarray}
	A = circ[a_o, a_1,\cdots,a_{n-1}] &=& \left[
		\begin{array}{lllll}
			a_0     & a_1     & a_2    & \cdots & a_{n-1} \\
			a_{n-1} & a_0     & a_1    & \cdots & a_{n-2} \\
			a_{n-2} & a_{n-1} & a_0    & \cdots & a_{n-3} \\
			\vdots  & \vdots  & \vdots & \ddots & \vdots  \\
			a_1     & a_2     & a_3    & \cdots & a_0
		\end{array}
	\right].
\end{eqnarray}

The eigenvalues of $A$ are given by
\begin{eqnarray*}
	\lambda_l & = & \sum_{k=0}^{n-1} a_k e^{i2lk\pi/n} \\
	& = & \sum_{k=0}^{n-1}a_k \rho_l^k, \quad \quad l = 0, 1, 2, \cdots, n-1,
\end{eqnarray*}

where $\rho_l = e^{i2\pi l/n}$ represents the $l$th root of 1, and the corresponding (unitary) eigenvector is given by
\[
	\psi_l = \frac{1}{\sqrt{n}}(1, \rho_l, \rho_l^2, \cdots, \rho_l^{n-1})^T.
\]

If matrix $A$ is real symmetric, that is, $a_i = a_{n-i}$, then its eigenvalues are real. More specifically, for even $n=2N$ the eigenvalues $\lambda_j = \lambda_{n-j}$ or there are either two eigenvalues or none with odd multiplicity, for odd $n=2N-1$ the eigenvalue $\lambda_0$ equal to any $\lambda_j$ for $1\le j \le N-1$ or $\lambda_0$ occurs with odd multiplicity. A square matrix $B$ is Hermitian, if and only if $B^* = B$ where $B^*$ is the complex conjugate. If $B$ is real then $B^* = B^T$. According to \cite{Tee2005} Hermitian matrices has a full set of orthogonal eigenvectors with corresponding real eigenvalues.



%-------------------------------------%
\subsection{\bf Block circulant matrices}
%-------------------------------------%

The idea of a block circulant matrix was first proposed by \cite{Muir1920}. A matrix $B_{np\times np}$ is a block-circulant matrix if it has the following form,


\begin{eqnarray}
	B = bcirc[A_o, A_1,\cdots,A_{n-1}] &=& \left[
		\begin{array}{lllll}
			A_0     & A_1     & A_2    & \cdots & A_{n-1} \\
			A_{n-1} & A_0     & A_1    & \cdots & A_{n-2} \\
			A_{n-2} & A_{n-1} & A_0    & \cdots & A_{n-3} \\
			\vdots  & \vdots  & \vdots & \ddots & \vdots  \\
			A_1     & A_2     & A_3    & \cdots & A_0
		\end{array}
	\right]
\end{eqnarray}

\noi where $A_j$ are ($p \times p$) sub-matrices of complex or real valued elements. \cite{DeMazancourt1983} proposed some methodologies to find the inverse of $B$. Let $M$ be a block-permutation matrix

\begin{eqnarray*}
	M = \left[
		\begin{array}{lllll}
			0       & I_p     & 0      & \cdots & 0 \\
			0       & 0       & I_p    & \cdots & 0 \\
			\vdots  & \vdots  & \vdots & \ddots & \vdots  \\
			0       & 0       & 0      & \cdots & I_p \\
			I_p     & 0     & 0    & \cdots & 0
		\end{array}
	\right],
\end{eqnarray*}

\noi where $I_p$ is $p\times p$ identity matrix and $B$ can be defined as follows,

\[
B = \sum_{k=0}^{n-1} A_k M^k.
\]

Define $M^0$ as ($np\times np$) identity matrix and the eigenvalues of $M$ given by $\rho_l$, the eigenmatrix of $M$ can be given by $Q_{np\times np}=\{ \utilde{\psi_0}, \utilde{\psi_1},\ldots, \utilde{\psi_{n-1}} \}$. From \cite{Trapp1973} it can be shown that $Q^{-1} = Q^*/n$ where $Q^*$ is the conjugate transpose of $Q$ now we can write,

\[
M = QDQ^{-1} = \frac{QDQ^*}{n}
\]

\noi where $D$ is a diagonal matrix and the diagonal elements $D_i \quad i=0,1,\ldots, n-1$ are the discrete Fourier transform of the blocks $A_j$,

\[
D_i = \sum_{k=0}^{n-1} A_k e^{i2lk\pi/n}.
\]

That is the inverse of matrix $B$ takes the following form,

\begin{eqnarray*}
	B^{-1} = Q\cdot\left(
		\begin{array}{llll}
			D_0^{-1}  & 0              & \cdots & 0 \\
			0         & D_1^{-1}       & \cdots & 0 \\
			\vdots    & \vdots         & \ddots & \vdots  \\
			0         & 0              & \cdots & D_n{-1}
		\end{array}
	\right)\cdot Q^{-1}.
\end{eqnarray*}

The eigenmatrix $Q$ is solely depending on the dimension of $B$ and the eigenvalues of $B$ ($\rho_l$'s), in other words, $B$ is not depending on the blocks ($A_j$'s), {\em i.e.} for any block diagonal matrix $D_{np \times np}$, $QDQ^{-1}$ is a block circulant matrix, and immediately follows that the inverse of the matrix $B$ is also a block circulant matrix.\\

When $A_{j_{1\times 1}}$, $B=A$, $D_i^{-1}=\lambda^{-1}$, and the eigenmatrix has a dimension of $n\times n$ then

\[
A^{-1} = Q \Lambda^{-1} Q^T \quad \mbox{where $\Lambda = \{\lambda_0, \ldots, \lambda_{n-1} \}$}
\]

When $A$ is real symmetric $Q$ is also real and symmetric and $Q^{-1}=Q^T$.\\


\blue{We need to add in the special case when $A_j$'s are symmetric (\blue{by Tee, add citation})}



%\bibliography{biblography}
%\end{document}
%
%%-------------------------------------%
%		\begin{thm}[Mercer's theorem (simplified version)] \label{mercer}  \hfill \\
%			%-------------------------------------%
%			
%			A kernal $K:[a,b]\times [a,b] \to \R$ be a symmetric continuous function that is non-negative definite,
%			
%			\[
%				\sum_{i=1}^{n}\sum_{j=1}^{n} a_i a_j K(s, t) \ge 0 \quad \mbox{and} \quad K(s,t) = K(t,s)
%			\]
%			
%			for all $(s,t)\in [a,b]$ and $a_i>0$. Let $T_K:L_2 \to L_2$ be an intergral operator defined by
%			
%			\[
%				[T_Kf](\cdot) = \int_{a}^{b} K(\cdot,s)f(s)ds
%			\]
%			
%			is positive, for all $f\in L_2$
%			
%			\[
%				\int_{a}^{b} K(s, t)f(s)f(t)dsdt \ge 0.
%			\]
%			
%			The corresponding orthonormal eigen functions $\psi_i\in L_2$ and non negative eigen values $\lambda_i \ge 0$ of the operator $T_K$ is defined as
%			
%			\[
%				T_K(\psi_i(\cdot)) = \int K(\cdot, s)\psi(s)ds = \lambda_i\psi_i(\cdot), \quad \int \psi_i(\cdot)\psi_j(\cdot) = \delta_{ij}
%			\]
%			
%			
%			then the kernal $K(\cdot)$ is a uniformly convergent series in terms of eigen functions and associated eigen values of $T_K$ as follows,
%			
%			\[
%				K(s,t) = \sum_{j=1}^{\infty} \lambda_i \psi_i(s)\psi_i(t)
%			\]
%			
%		\end{thm}
%
